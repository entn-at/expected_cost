{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook computes and plots the results in Table 1\n",
    "\n",
    "import numpy as np\n",
    "from expected_cost import ec, utils\n",
    "from expected_cost.data import get_llks_for_multi_classif_task\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = '../data/cifar10_resnet-50/'\n",
    "dataset = 'gaussian_sim'\n",
    "\n",
    "if dataset == 'gaussian_sim':\n",
    "    # Simulation parameters\n",
    "    N = 10000       # total number of samples\n",
    "    var = 0.20      # within-class standard deviation of the features, determines the difficulty of the problem\n",
    "    P0 = 0.8        # Prior for class 0, 0.8 or 0.5 for the results in the paper\n",
    "    K = 10          # Number of classes\n",
    "    priors = np.array([P0] + [(1 - P0) / (K - 1)] * (K - 1))\n",
    "else:\n",
    "    priors, var, N = None, None, None\n",
    "    std = None\n",
    "\n",
    "# Generate or read the log posteriors\n",
    "\n",
    "targets, _, logpost = get_llks_for_multi_classif_task(dataset, priors=priors, sim_params={'feat_var': var}, N=N, logpost=True)\n",
    "\n",
    "if dataset != 'gaussian_sim':\n",
    "    N = len(targets)\n",
    "    K = logpost.shape[1]\n",
    "    priors = np.bincount(targets)/N\n",
    "\n",
    "# Define various costs matrices\n",
    "costs = {}\n",
    "\n",
    "# Standard 0-1 cost\n",
    "costs['C01'] = ec.CostMatrix.zero_one_costs(K)\n",
    "\n",
    "# Balanced error rate. The costs are inversely proportional to the priors.\n",
    "costm = (1 - np.eye(K))/np.atleast_2d(priors).T/K\n",
    "costs['CinvP']  = ec.CostMatrix(costm)\n",
    "\n",
    "# A 0-1 cost with the last rwo replaced by 100 to simulate a case in which the errors in that class\n",
    "# are much more costly than the errors in other classes.\n",
    "costm = 1 - np.eye(K)\n",
    "costm[-1,:] *= 100\n",
    "costs['Cimp']  = ec.CostMatrix(costm)\n",
    "\n",
    "# Finally, two cost functions with abstention options with different cost.\n",
    "costs['Cabs1'] = ec.CostMatrix.zero_one_costs(K, abstention_cost=0.05)\n",
    "costs['Cabs2'] = ec.CostMatrix.zero_one_costs(K, abstention_cost=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best naive decision for C01    : 1\n",
      "Best naive decision for CinvP  : 9\n",
      "Best naive decision for Cimp   : 10\n",
      "Best naive decision for Cabs1  : 11\n",
      "Best naive decision for Cabs2  : 1\n",
      "\n",
      "Decisions    C01            CinvP          Cimp           Cabs1         %Abs    Cabs2         %Abs   \n",
      "Naive        0.20   1.00    0.90   1.00    0.98   1.00    0.05   1.00    100    0.20   1.00      0  \n",
      "Argmax       0.06   0.32    0.28   0.31    0.36   0.37    0.06   1.29      0    0.06   0.32      0  \n",
      "Bayes        0.06   0.32    0.23   0.26    0.08   0.08    0.02   0.35     25    0.06   0.28      7  \n"
     ]
    }
   ],
   "source": [
    "# Print the best naive decision for each cost matrix\n",
    "\n",
    "for costn, cost in costs.items():\n",
    "    naive_dec = np.argmin(np.dot(priors.T, cost.get_matrix()))+1\n",
    "    print(f\"Best naive decision for {costn:7s}: {naive_dec}\")\n",
    "print(\"\")\n",
    "\n",
    "# Table Header\n",
    "\n",
    "sep = '  ' # Field separator for printing \n",
    "print(f\"Decisions{sep:s} \", end='')\n",
    "for costn, cost in costs.items():\n",
    "    print(f\" {costn:10s}  {sep:s}\", end='')\n",
    "    if 'abs' in costn:\n",
    "        print(f\"%Abs {sep:s}\", end='')\n",
    "print(\"\")\n",
    "\n",
    "# Argmax decisions, which are the same for all cost matrices\n",
    "\n",
    "argmax_decisions = np.argmax(logpost, axis=-1)\n",
    "    \n",
    "# Print the various ECs for each of the decision algorithms     \n",
    "    \n",
    "for dec in ['Naive', 'Argmax', 'Bayes']:\n",
    "\n",
    "    print(f\"{dec:6s}   {sep:s}\", end='')\n",
    "    \n",
    "    for costn, cost in costs.items():\n",
    "\n",
    "        if dec == 'Argmax':\n",
    "            decisions = argmax_decisions\n",
    "        elif dec == 'Bayes':\n",
    "            decisions, _ = ec.bayes_decisions(logpost, cost, score_type='log_posteriors')\n",
    "        elif dec == 'Naive':\n",
    "            decisions = np.ones_like(targets) * np.argmin(np.dot(priors.T, cost.get_matrix())) \n",
    "        else:\n",
    "            print(\"Unknown decision algorithm\")\n",
    "            continue\n",
    "        \n",
    "        ecval  = ec.average_cost(targets, decisions, cost, adjusted=False)\n",
    "        ecvaln = ec.average_cost(targets, decisions, cost, adjusted=True)\n",
    "        \n",
    "        print(f\" {ecval:5.2f}{sep:s}{ecvaln:5.2f}{sep:s}\", end='')\n",
    "        norm = np.min(np.dot(priors.T, cost.get_matrix()))\n",
    "        if 'abs' in costn:\n",
    "            perc_abs = np.sum(decisions == K) / len(decisions) * 100\n",
    "            print(f\"{perc_abs:5.0f}{sep:s}\", end='')\n",
    "    \n",
    "    print('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
